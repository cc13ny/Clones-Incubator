1
00:00:00,261 --> 00:00:05,070
[音乐]

2
00:00:05,070 --> 00:00:09,050
大家好 
欢迎再次回到异质并行编程课

3
00:00:09,050 --> 00:00:14,230
我们在6.3课时 
《高效的“主机-设备”数据传输》

4
00:00:14,230 --> 00:00:15,300
我们将会去

5
00:00:15,300 --> 00:00:19,610
讨论兼有计算的重叠数据传输

6
00:00:19,610 --> 00:00:21,600
这节课的目标是为了让你

7
00:00:21,600 --> 00:00:25,650
学会兼有计算的重叠数据传输

8
00:00:25,650 --> 00:00:30,770
重叠的数据交流
或者兼有计算的传输

9
00:00:30,770 --> 00:00:34,830
是大规模并行计算的根本问题

10
00:00:34,830 --> 00:00:40,210
然后你将会在其他编程系统里
有着类型完全相同的问题

11
00:00:40,210 --> 00:00:45,740
譬如编程系统 MPI (信息传递接口)
或者叫 MPI Program (MPI程序)

12
00:00:45,740 --> 00:00:51,370
所以今天我们将会关注
如何地去处理这些潜在的

13
00:00:51,370 --> 00:00:56,510
在CUDA环境下的
重叠数据传输和计算

14
00:00:56,510 --> 00:00:58,765
通过异步数据传输(解决)

15
00:00:58,765 --> 00:01:04,120
而且也会讨论CUDA流的
一些实际的限制

16
00:01:05,242 --> 00:01:11,900
但是我们现在已经在前一章课程里
讨论了CUDA流的概念

17
00:01:11,900 --> 00:01:18,110
我们现在为在CUDA主机代码里
流的实际使用已经准备好

18
00:01:18,110 --> 00:01:22,560
所以在这我们展示流控的声明

19
00:01:22,560 --> 00:01:27,032
这里有个特殊的类型cudaStream_t

20
00:01:27,032 --> 00:01:31,140
我们可以声明
两个指向流的流控或者指针

21
00:01:31,140 --> 00:01:34,310
我们把它们称为stream0和stream1

22
00:01:34,310 --> 00:01:37,610
一旦我们声明了
这两个变量 我们可以

23
00:01:37,610 --> 00:01:42,680
调用cudaStreamCreate
把这些变量的地址作为参数


24
00:01:42,680 --> 00:01:48,520
而这个应用程序
编程接口的调用会

25
00:01:48,520 --> 00:01:53,340
产生一个CUDA流 
产生合适的线索 然后把

26
00:01:53,340 --> 00:01:58,440
指向流或者线索的
指针放在stream0和stream1里

27
00:01:58,440 --> 00:02:03,340
所以当我们在将来使用
内存复制和内核启动

28
00:02:03,340 --> 00:02:08,430
诸如此类的  我们可用
两个流中的其中一个去

29
00:02:08,430 --> 00:02:13,980
表明我们想
向哪个流发行任务

30
00:02:13,980 --> 00:02:14,716
或者命令

31
00:02:14,716 --> 00:02:21,160
在这我们展示
我们可以声明给

32
00:02:21,160 --> 00:02:26,990
A, B和C的向量部分 
那些我们将要用来或者分割

33
00:02:26,990 --> 00:02:32,220
以之实现沟通和计算的重叠

34
00:02:32,220 --> 00:02:34,615
所以在这我们展示一段实用的

35
00:02:34,615 --> 00:02:39,180
多流主机代码来实现


36
00:02:39,180 --> 00:02:44,080
数据传输和计算之间
一定程度的重叠

37
00:02:44,080 --> 00:02:47,960
所在在这我们展示
我们分割

38
00:02:47,960 --> 00:02:53,710
所有的输入和输出向量
成等量的分块

39
00:02:53,710 --> 00:02:58,350
每个分块的大小
被定义到变量SegSize里

40
00:02:59,460 --> 00:03:04,220
我们将会涵括数据传输和计算

41
00:03:04,220 --> 00:03:07,650
到其中两个分块
在这循环的每次迭代里

42
00:03:07,650 --> 00:03:12,024
所有如果我们
把这向量分割到 譬如说

43
00:03:12,024 --> 00:03:17,148
100份分块 然后我们将会
迭代这个循环50次

44
00:03:17,148 --> 00:03:23,442
在这循环的每次迭代里
我们将会取出两个分块

45
00:03:23,442 --> 00:03:27,590
把它们送到GPU（图处理单元）里
去计算和把它们送回到结果数据里

46
00:03:27,590 --> 00:03:29,400
在这我们展示数据

47
00:03:29,400 --> 00:03:34,070
复制从主机到设备 
和我们正在用cudaMemcpyAsync

48
00:03:34,070 --> 00:03:37,900
这是异步数据传输

49
00:03:37,900 --> 00:03:42,390
我们早前讲过了一些

50
00:03:42,390 --> 00:03:47,940
然后我们也需要确定
当我们分配h_A

51
00:03:47,940 --> 00:03:53,336
和h_B时 我们需要把它们
分配到固定主机内存


52
00:03:53,336 --> 00:03:55,380
然后我们将会

53
00:03:55,380 --> 00:04:00,190
只是用i去实际上
推进这指针

54
00:04:00,190 --> 00:04:05,160
到特定的分块 
这分块是我们正在送到GPU的

55
00:04:05,160 --> 00:04:11,180
我们进行从主机到设备的数据复制
然后我们启动一个内核 接着我们

56
00:04:11,180 --> 00:04:16,870
把数据从设备复制回主机
全部通过stream0的流控

57
00:04:16,870 --> 00:04:20,695
所以全部这些任务会进到队列里

58
00:04:20,695 --> 00:04:21,702
为了stream0

59
00:04:21,702 --> 00:04:27,580
接着这次迭代的第二部分
我们将会对下一分块做同样的事情

60
00:04:27,580 --> 00:04:34,210
但是我们将会提交这些
指令或任务到stream1里面

61
00:04:34,210 --> 00:04:37,730
所以这是为什么
在每次迭代中我们关注

62
00:04:37,730 --> 00:04:43,130
两个分块 然后我们在i上
增加两个分块的大小

63
00:04:43,130 --> 00:04:45,738
接着我们继续
直到我们处理完所有数据

64
00:04:45,738 --> 00:04:51,640
在一些更古老的GPU里

65
00:04:51,640 --> 00:04:55,810
实际上在执行中并没有流队列

66
00:04:55,810 --> 00:04:58,860
反而这些队列事实上

67
00:05:00,150 --> 00:05:03,800
关联着复制引擎和内核引擎

68
00:05:03,800 --> 00:05:10,780
复制引擎是一种硬件 
它执行PCIe（外围组件互连快线）

69
00:05:10,780 --> 00:05:14,650
数据总线的传输 向上意味着
（数据）从设备到主机

70
00:05:14,650 --> 00:05:17,840
或者向下意味着
（数据）从主机到设备

71
00:05:17,840 --> 00:05:22,210
所以这实际上是我们在6.1课时说到的
DMA(直接存储器存取) 硬件


72
00:05:22,210 --> 00:05:28,410
并且有个队列和它相连
一个真的硬件队列和它相连

73
00:05:28,410 --> 00:05:31,534
接着那里有个内核引擎

74
00:05:31,534 --> 00:05:35,865
你也有着内核任务
内核启动任务和内核引擎相连的

75
00:05:35,865 --> 00:05:40,537
内核引擎事实上只是一个机制 

76
00:05:40,537 --> 00:05:45,939
它包含所有用于执行
CUDA内核的流式复数处理器

77
00:05:45,939 --> 00:05:50,646
所以当我们提交这些任务

78
00:05:50,646 --> 00:05:55,605
到概念上的流式队列里
在那几代

79
00:05:55,605 --> 00:06:01,695
硬件里时 发生的是
我们事实上将会把

80
00:06:01,695 --> 00:06:06,808
内存数据传输任务
放到复制引擎队列里

81
00:06:06,808 --> 00:06:10,920
然后我们进入所有在内核引擎
队列的内核启动

82
00:06:10,920 --> 00:06:15,470
所以我们正弄混了

83
00:06:15,470 --> 00:06:19,930
来自两个不同流的数据传输任务

84
00:06:19,930 --> 00:06:27,300
来自stream0的和来自stream1的
然而我们将会允许并行

85
00:06:27,300 --> 00:06:33,600
执行于这两个引擎之间 
复制引擎和内核引擎 

86
00:06:33,600 --> 00:06:39,730
通过保持注意这些队列里
项之间的依赖

87
00:06:39,730 --> 00:06:45,373
举个例子 内核启动0依靠着

88
00:06:45,373 --> 00:06:49,250
从主机到设备的数据复制A0和B0


89
00:06:49,250 --> 00:06:52,410
所以这里有个依赖性箭头从

90
00:06:52,410 --> 00:06:57,920
内核0到这两个（MemCpy)
所以当它们俩到达

91
00:06:57,920 --> 00:07:03,020
队列的开头


92
00:07:03,020 --> 00:07:08,030
内核0将只能够执行
当两个内存复制


93
00:07:08,030 --> 00:07:13,650
A0和B0已经完成它们的执行

94
00:07:13,650 --> 00:07:17,650
所以这允许硬件去保持注意到


95
00:07:17,650 --> 00:07:23,020
一个事实
内核启动0实际上进入了


96
00:07:23,020 --> 00:07:28,010
同一个流里
就像内存复制A0和

97
00:07:28,010 --> 00:07:33,060
内存复制B0一样
所以这允许我们有能力去

98
00:07:34,120 --> 00:07:39,160
以并行的方式做这些任务
同时仍然保留着

99
00:07:39,160 --> 00:07:44,490
stream0里原来的序列顺序

100
00:07:44,490 --> 00:07:50,950
在这我们有着同样的事
内核1依靠着内存

101
00:07:50,950 --> 00:07:57,636
复制A1和BI 
然后往回的复制C1将依靠着内核1

102
00:07:57,636 --> 00:08:04,250
所以 因为我们现在有个硬件队列

103
00:08:04,250 --> 00:08:09,670
在复制引擎之后 所以所有这些项
会从备份队列中被释放

104
00:08:09,670 --> 00:08:14,140
（从复制引擎中按顺序）
所以我们实际上有着一种情形

105
00:08:14,140 --> 00:08:19,190
一个stream1任务会被

106
00:08:19,190 --> 00:08:24,360
其中一个stream0任务挡住
这是实现的选择

107
00:08:25,730 --> 00:08:30,450
所以在这特殊的设计里
我们也可以有

108
00:08:30,450 --> 00:08:34,880
一个内存复制B 譬如说C0

109
00:08:34,880 --> 00:08:39,350
在PCIe的上面
然后我们可以立即释放

110
00:08:39,350 --> 00:08:44,290
内存复制A1到PCIe的下面

111
00:08:44,290 --> 00:08:49,670
只要

112
00:08:49,670 --> 00:08:54,571
我们有着内存 所有主要复制
A0，B0和C0

113
00:08:54,571 --> 00:09:00,220
已经要不被执行要不就在进行中

114
00:09:00,220 --> 00:09:02,410
在实际往上或者往下的链接里

115
00:09:02,410 --> 00:09:06,040
所以这允许我们一定程度上的并行

116
00:09:06,040 --> 00:09:09,360
而我们事实上将会在这看到

117
00:09:09,360 --> 00:09:14,410
所以通过这个复制引擎队列和这个

118
00:09:14,410 --> 00:09:17,700
内核队列 
我们将会事实上拥有一个执行

119
00:09:17,700 --> 00:09:20,720
时序 而它并不是非常符合我们所希望的

120
00:09:20,720 --> 00:09:24,370
在我们早前展示过的理想时序图
（里面的情形）

121
00:09:24,370 --> 00:09:25,310
所以在这我们

122
00:09:25,310 --> 00:09:28,094
展示在一次循环迭代中

123
00:09:28,094 --> 00:09:31,068
我们将会拥有从主机到设备的

124
00:09:31,068 --> 00:09:33,450
数据复制 接着我们计算

125
00:09:33,450 --> 00:09:37,710
和数据从设备（复制）回到主机的传输

126
00:09:37,710 --> 00:09:42,970
所以这些从设备返回到

127
00:09:42,970 --> 00:09:48,220
主机的数据传输实际上挡住了
在队列中A1和B1的传输

128
00:09:48,220 --> 00:09:50,850
这个往回复制的

129
00:09:50,850 --> 00:09:55,100
任务在等待着来自同一个流的计算

130
00:09:55,100 --> 00:09:59,920
它事实上挡住了所有
在复制引擎队列中在它后面的任务

131
00:09:59,920 --> 00:10:05,460
所以我们不能释放

132
00:10:05,460 --> 00:10:10,810
A1和B1的传输
直到C的传输被释放到

133
00:10:10,810 --> 00:10:15,990
PCIe上面的硬件 
那是我们能够开始执行

134
00:10:15,990 --> 00:10:20,180
A1和B1传输的时候
所以我们有一些重叠

135
00:10:20,180 --> 00:10:26,230
有着很多重叠存在于
下个从主机到设备的实际元素块的

136
00:10:26,230 --> 00:10:30,640
数据传输（诸如Trans A.1）同时（存在于)
我们把数据从设备传输回主机 （诸如Trans C.0）

137
00:10:30,640 --> 00:10:35,760
但是我们并没有我们所希望实现的
兼有计算重叠的数据传输

138
00:10:35,760 --> 00:10:41,040
在这代码段里
所以在这我们展示

139
00:10:41,040 --> 00:10:44,560
一段更好的多流式主机代码
它将会实现

140
00:10:44,560 --> 00:10:46,890
我们所希望更多的（计算）重叠

141
00:10:46,890 --> 00:10:52,783
所以在同一次循环迭代里 我们实际上
重新调整了内存复制的顺序

142
00:10:52,783 --> 00:10:59,483
以及内核基本上交织了
从主机到设备的数据复制

143
00:10:59,483 --> 00:11:06,070
内核启动以及在两个流之间
从设备（复制）回来的数据复制

144
00:11:06,070 --> 00:11:09,696
所以我们将要通过stream0从
主机复制数据到设备

145
00:11:09,696 --> 00:11:14,840
和立即地 我们将要通过stream1
从设备复制数据到主机

146
00:11:14,840 --> 00:11:19,230
然后接着是对这两个流的内核启动

147
00:11:19,230 --> 00:11:22,620
然后我们将会（把数据）
复制回到两个流

148
00:11:22,620 --> 00:11:27,108
所以这个顺序 
这个调用顺序将会允许这些任务

149
00:11:27,108 --> 00:11:31,156
去进入这复制引擎的队列里

150
00:11:31,156 --> 00:11:37,342
以不一样的顺序
这顺序允许我们有更多的重叠

151
00:11:37,342 --> 00:11:43,122
所以这展示了这实际顺序里
从主机到设备的数据复制A0 B0

152
00:11:43,122 --> 00:11:49,280
立即被从主机到设备的
数据复制A1和B1跟上

153
00:11:49,280 --> 00:11:53,200
然后它们会被跟随着
从C0和C1返回的数据复制

154
00:11:54,240 --> 00:11:56,830
这里仍然有同样类型的依赖

155
00:11:56,830 --> 00:12:04,640
从内核0到数据复制A0和B0
这里仍有一样的

156
00:12:04,640 --> 00:12:11,330
依赖
从数据复制C0到内核0的执行

157
00:12:11,330 --> 00:12:15,430
但是 在这复制引擎队列里面 
顺序已经被改变了

158
00:12:15,430 --> 00:12:20,500
因为我们在循环里面改变了
API（应用编程接口）调用的顺序

159
00:12:20,500 --> 00:12:22,360
所以 这允许

160
00:12:22,360 --> 00:12:29,500
数据复制A1和B1去进入PCIe的下面

161
00:12:29,500 --> 00:12:35,440
引擎
而且并不会被从C0回来的数据复制挡住

162
00:12:35,440 --> 00:12:39,770
所以这被反映到时序图里

163
00:12:39,770 --> 00:12:45,640
这里我们展示了在一次迭代里面
两个分块执行的时序

164
00:12:45,640 --> 00:12:47,700
所以在这我们看到

165
00:12:47,700 --> 00:12:53,800
一旦在下面的复制引擎
完成了对B0的复制 你将会有能力去

166
00:12:53,800 --> 00:12:58,640
立即执行内存数据传输


167
00:12:58,640 --> 00:13:01,500
把A1和B1的数据从主机传输到设备

168
00:13:01,500 --> 00:13:06,912
所以这实现了
我们在早前展现的代码段里


169
00:13:06,912 --> 00:13:13,290
不能实现的计算和数据传输的重叠

170
00:13:13,290 --> 00:13:17,490
并且对于现在C传输回到主机也

171
00:13:17,490 --> 00:13:21,060
和A1和B1的计算重叠

172
00:13:22,500 --> 00:13:28,140
不幸的是 当我们移动下次迭代中
下次迭代中

173
00:13:28,140 --> 00:13:33,540
A2和B2的复制将
仍然被（复制）回到主机的

174
00:13:33,540 --> 00:13:38,320
C1数据传输挡住 因为C1会

175
00:13:38,320 --> 00:13:41,380
需要等待直到计算结束

176
00:13:41,380 --> 00:13:46,690
所以A2和B2的复制仍然将会有着

177
00:13:46,690 --> 00:13:51,680
与A1和B1计算重叠的困难

178
00:13:51,680 --> 00:13:56,160
所以我介绍的这代码段
解决了部分问题

179
00:13:56,160 --> 00:13:58,610
增加了在计算和沟通

180
00:13:58,610 --> 00:14:02,530
（也就是）数据传输之间
重叠的程度


181
00:14:02,530 --> 00:14:03,610
但是在

182
00:14:03,610 --> 00:14:08,140
每次循环迭代的边界上 当我们
移到下次迭代中 我们

183
00:14:08,140 --> 00:14:12,710
将会一直有着我们希望实现的
少于一次重叠的部分

184
00:14:13,770 --> 00:14:19,734
所以为了实现一个理想的时序管道
我们实际上需要

185
00:14:19,734 --> 00:14:22,422
对于每次循环迭代

186
00:14:22,422 --> 00:14:27,760
基本上提交三次流

187
00:14:27,760 --> 00:14:28,700
所以我们

188
00:14:28,700 --> 00:14:32,550
可以有着类似这个的情形

189
00:14:32,550 --> 00:14:37,780
但是我们真正需要做的 你会知道的

190
00:14:37,780 --> 00:14:43,530
如果你尝试在你的实验MP
（机器问题）中获得可能的最好表现

191
00:14:43,530 --> 00:14:49,250
你所需要做的是 你实际需要写个一个
循环 和在循环的外面

192
00:14:49,250 --> 00:14:53,852
你将会执行A0 B0的传输 以及
你将会实际调用A0 B0的

193
00:14:53,852 --> 00:14:58,142
计算
以及执行A1和B1的传输

194
00:14:58,142 --> 00:15:05,370
在循环的外面 在任务或函数调用
在你进入循环之前

195
00:15:05,370 --> 00:15:08,040
然后当你进入这个循环的时候
你将会

196
00:15:08,040 --> 00:15:14,330
实际上调用C0的往回数据传输

197
00:15:14,330 --> 00:15:16,380
然后你会执行A1和B1的计算

198
00:15:16,380 --> 00:15:19,130
接着你将会把传输来自A2和B2的数据

199
00:15:19,130 --> 00:15:24,130
全部都在同一次循环迭代中
以及你将会用一个

200
00:15:24,130 --> 00:15:29,674
变量去

201
00:15:29,674 --> 00:15:34,960
实际上表明你正在用哪个流

202
00:15:34,960 --> 00:15:39,020
然后你将会为这个变量（逐步）增量


203
00:15:39,020 --> 00:15:44,160
当你进到下次迭代里面 
那么你将会用到下一个

204
00:15:44,160 --> 00:15:50,260
组合
所以这是高度的复杂

205
00:15:50,260 --> 00:15:55,610
非常复杂 
我们称它为“硫管道”代码段

206
00:15:55,610 --> 00:15:59,670
除非你真的 真的很感兴趣

207
00:15:59,670 --> 00:16:02,840
我们将不会推荐你去尝试它

208
00:16:02,840 --> 00:16:05,720
所以我会推荐你去尝试
更简单的代码段

209
00:16:05,720 --> 00:16:09,536
那些我们介绍过的代码段
至少实现了

210
00:16:09,536 --> 00:16:14,504
两个分块的重叠
接着一个你能做的简单执行是


211
00:16:14,504 --> 00:16:19,652
你可以有一个相对大的数字 
譬如说八条流

212
00:16:19,652 --> 00:16:24,901
你可以用一个数组去存
所有（流）控

213
00:16:24,901 --> 00:16:30,049
然后当你做循环迭代 
你只是逐步增加了流的索引

214
00:16:30,049 --> 00:16:35,197
那么你将会有三个连续的运作的流

215
00:16:35,197 --> 00:16:37,230
在每次循环迭代中

216
00:16:37,230 --> 00:16:40,698
但是我依然会鼓励你去

217
00:16:40,698 --> 00:16:45,480
完成更简单的版本 
在你尝试更复杂的版本之前

218
00:16:46,640 --> 00:16:51,570
因为这复杂度 新一代的硬件

219
00:16:51,570 --> 00:16:55,415
将不会或者说实际上执行了
被我们称之为混合队列

220
00:16:55,415 --> 00:17:00,360
然后在混合队列里 发生的是


221
00:17:00,360 --> 00:17:03,950
在每个引擎之前 
无论它是复制引擎或者内核引擎

222
00:17:03,950 --> 00:17:09,150
我们实际拥有复数的真实的队列

223
00:17:09,150 --> 00:17:15,340
所以我们有流进入到这些引擎

224
00:17:15,340 --> 00:17:19,740
我们将会有能力去存储那些任务

225
00:17:19,740 --> 00:17:24,000
它们属于不同流并且在
引擎前不同的硬件队列里

226
00:17:24,000 --> 00:17:25,475
所以你可以想象存在着

227
00:17:25,475 --> 00:17:30,018
多个引擎在复制引擎前面的队列里
以及多个队列在

228
00:17:30,018 --> 00:17:34,830
内核引擎的前面
所以这允许硬件

229
00:17:34,830 --> 00:17:40,030
从队列的开头去选择 那么

230
00:17:40,030 --> 00:17:45,010
在其中一个流中的操作
不会偶然挡住另外一个

231
00:17:45,010 --> 00:17:50,730
在另一队列里的操作
所以这基本上允许我们去实现

232
00:17:50,730 --> 00:17:54,230
全重叠 即使是通过更简单的代码段

233
00:17:54,230 --> 00:17:58,830
在第一版本里最简单的代码段里

234
00:17:58,830 --> 00:18:03,850
我们只是基本上提交了所有操作


235
00:18:03,850 --> 00:18:06,110
到一个流里 到一次迭代中 
然后到下一个

236
00:18:06,110 --> 00:18:11,410
甚至那个会很有可能
与混合队列硬件工作得很好

237
00:18:11,410 --> 00:18:16,100
从开普勒（Kepler）GPU世代开始
现在的GPU

238
00:18:16,100 --> 00:18:21,260
装备上了混合队列 
而且将会很有可能持续增加

239
00:18:21,260 --> 00:18:25,110
在每个引擎前队列的数量
当硬件改善的时候

240
00:18:25,110 --> 00:18:31,060
我想再做个一个快速评论
关于真正的好处

241
00:18:31,060 --> 00:18:38,340
去为了向量相加
去实现复杂的管道执行

242
00:18:38,340 --> 00:18:41,220
结果最终发现向量相加内核实际上

243
00:18:41,220 --> 00:18:43,250
执行得速度极快

244
00:18:43,250 --> 00:18:48,390
去执行向量相加内核的时间
大概实际上是十分之一

245
00:18:48,390 --> 00:18:54,520
或者甚至少于数据传输
十分之一（的时间）

246
00:18:54,520 --> 00:18:57,660
所以我们实际上在这正有着极小

247
00:18:57,660 --> 00:19:02,310
极度极度小的计算时间

248
00:19:02,310 --> 00:19:05,050
所以这对于向量相加来说是不成比例的

249
00:19:05,050 --> 00:19:06,170
那么 在

250
00:19:06,170 --> 00:19:10,680
如果我们回到上一张关于重叠的图片

251
00:19:10,680 --> 00:19:16,270
真正发生的是当我们有着非常小的
计算分块的时候

252
00:19:16,270 --> 00:19:22,210
然后所有我们在实际上真的可做的是
真的只是去重叠

253
00:19:22,210 --> 00:19:24,630
往主机（复制）回的数据传输和

254
00:19:24,630 --> 00:19:28,040
在下一分块里到设备的数据传输


255
00:19:28,040 --> 00:19:31,480
所以你将会看到向量相加

256
00:19:31,480 --> 00:19:36,270
如果你执行全部的版本
你将会看到即使是最简单的代码段

257
00:19:36,270 --> 00:19:39,658
（也）会实际上令你得到非常多的

258
00:19:39,658 --> 00:19:42,640
全部的你可为向量相加拥有的好处

259
00:19:42,640 --> 00:19:46,690
这是因为那极小量

260
00:19:46,690 --> 00:19:49,680
我们用在向量相加内核的时间

261
00:19:49,680 --> 00:19:54,280
那么这把我们带到了这节课的最后

262
00:20:00,180 --> 00:20:06,440
但是我想提一下一个重要


263
00:20:06,440 --> 00:20:11,578
API（应用程序接口）函数集
你会把它用到异步数据传输

264
00:20:11,578 --> 00:20:15,610

一个是cudaStreamSynchronize 
而这个函数有着

265
00:20:15,610 --> 00:20:19,910
一个输入参数（一个流的识别码）

266
00:20:19,910 --> 00:20:25,760
和这个将会强制主机代码去等待
直到所有在一个流的任务

267
00:20:25,760 --> 00:20:27,280
完成为止

268
00:20:27,280 --> 00:20:29,822
所以如果你想确定

269
00:20:29,822 --> 00:20:33,070
主机代码一直等待

270
00:20:33,070 --> 00:20:36,790
目前为止所有在流里的东西

271
00:20:36,790 --> 00:20:39,910
已经完成在主机代码继续之前

272
00:20:39,910 --> 00:20:42,240
一般来说 这是用来给诸如

273
00:20:42,240 --> 00:20:46,140
信息传递编码 MPI(信息传递接口)
CUDA实现和诸如此类

274
00:20:46,140 --> 00:20:51,380
这是个好用的指令


275
00:20:51,380 --> 00:20:53,250
而这里有个例子 
我们可以调用CUDA字符串

276
00:20:53,250 --> 00:20:57,050
与stream0同步 而这将会强制

277
00:20:57,050 --> 00:21:03,900
主机代码去等待直到所有在stream0
队列中的任务去完成

278
00:21:03,900 --> 00:21:06,160
而这里有另一个甚至

279
00:21:06,160 --> 00:21:09,580
更普遍的指令叫 
cudaDeviceSynchronize

280
00:21:09,580 --> 00:21:12,110
它也可以被用在主机代码

281
00:21:12,110 --> 00:21:17,020
它没有任何参数 
而且它会等待直到所有

282
00:21:17,020 --> 00:21:20,650
在全部流里的任务去完成

283
00:21:20,650 --> 00:21:23,910
在主机代码可以继续执行之前

284
00:21:23,910 --> 00:21:27,280
所以这被用来任何时候
你真的真的很想去确定

285
00:21:27,280 --> 00:21:34,060
你所有之前在全部流中的异步操作

286
00:21:34,060 --> 00:21:37,830
已经完成执行
在主机代码将要尝试去使用

287
00:21:37,830 --> 00:21:41,200
那数据结构或者尝试去
做下次批处理之前

288
00:21:43,030 --> 00:21:48,490
它把我们带到第六周的终点
而现在你已经准备好


289
00:21:48,490 --> 00:21:53,980
去做实验任务或者流式化向量相加


290
00:21:53,980 --> 00:21:57,530
对于你们那些喜欢学习更多关于

291
00:21:57,530 --> 00:21:59,850
异步数据传输的使用以及

292
00:21:59,850 --> 00:22:03,950
数据传输交流和计算的重叠

293
00:22:03,950 --> 00:22:08,400
我乐意推荐你们去阅读
Jason Sanders和


294
00:22:08,400 --> 00:22:12,420
Edward Kandrot 那里有一本优秀的书

295
00:22:13,850 --> 00:22:18,780
《CUDA By Example, 
An Introduction to General-Purpose GPU Programming》

296
00:22:18,780 --> 00:22:24,250
这本书带有非常好的例子

297
00:22:24,250 --> 00:22:29,340
根据教授如何去写并行CUDA代码

298
00:22:29,340 --> 00:22:33,820
而其中一个章节给了非常非常好的解释

299
00:22:33,820 --> 00:22:38,340
关于CUDA流以及它的实际使用


300
00:22:38,340 --> 00:22:38,690
谢谢你们
